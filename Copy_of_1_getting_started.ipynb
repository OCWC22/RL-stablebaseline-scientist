{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OCWC22/RL-stablebaseline-scientist/blob/main/Copy_of_1_getting_started.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyyN-2qyK_T2"
      },
      "source": [
        "# Stable Baselines3 Tutorial - Getting Started\n",
        "\n",
        "Github repo: https://github.com/araffin/rl-tutorial-jnrr19\n",
        "\n",
        "Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n",
        "\n",
        "Documentation: https://stable-baselines.readthedocs.io/en/master/\n",
        "\n",
        "RL Baselines3 zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n",
        "\n",
        "\n",
        "[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a training framework for Reinforcement Learning (RL), using Stable Baselines3.\n",
        "\n",
        "It provides scripts for training, evaluating agents, tuning hyperparameters, plotting results and recording videos.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook, you will learn the basics for using stable baselines3 library: how to create a RL model, train it and evaluate it. Because all algorithms share the same interface, we will see how simple it is to switch from one algorithm to another.\n",
        "\n",
        "\n",
        "## Install Dependencies and Stable Baselines3 Using Pip\n",
        "\n",
        "List of full dependencies can be found in the [README](https://github.com/DLR-RM/stable-baselines3).\n",
        "\n",
        "\n",
        "```\n",
        "pip install stable-baselines3[extra]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7Tgosr7VBo-l"
      },
      "outputs": [],
      "source": [
        "# for autoformatting\n",
        "# %load_ext jupyter_black"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gWskDE2c9WoN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: apt-get\n",
            "Requirement already satisfied: stable-baselines3[extra]>=2.0.0a4 in ./.venv-sb3/lib/python3.9/site-packages (2.6.0)\n",
            "Requirement already satisfied: matplotlib in ./.venv-sb3/lib/python3.9/site-packages (from stable-baselines3[extra]>=2.0.0a4) (3.9.4)\n",
            "Requirement already satisfied: cloudpickle in ./.venv-sb3/lib/python3.9/site-packages (from stable-baselines3[extra]>=2.0.0a4) (3.1.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in ./.venv-sb3/lib/python3.9/site-packages (from stable-baselines3[extra]>=2.0.0a4) (2.0.2)\n",
            "Requirement already satisfied: pandas in ./.venv-sb3/lib/python3.9/site-packages (from stable-baselines3[extra]>=2.0.0a4) (2.2.3)\n",
            "Requirement already satisfied: gymnasium<1.2.0,>=0.29.1 in ./.venv-sb3/lib/python3.9/site-packages (from stable-baselines3[extra]>=2.0.0a4) (1.1.1)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in ./.venv-sb3/lib/python3.9/site-packages (from stable-baselines3[extra]>=2.0.0a4) (2.7.0)\n",
            "Collecting opencv-python\n",
            "  Downloading opencv_python-4.11.0.86-cp37-abi3-macosx_13_0_arm64.whl (37.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 37.3 MB 13.4 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: pillow in ./.venv-sb3/lib/python3.9/site-packages (from stable-baselines3[extra]>=2.0.0a4) (11.2.1)\n",
            "Requirement already satisfied: tqdm in ./.venv-sb3/lib/python3.9/site-packages (from stable-baselines3[extra]>=2.0.0a4) (4.67.1)\n",
            "Collecting pygame\n",
            "  Downloading pygame-2.6.1-cp39-cp39-macosx_11_0_arm64.whl (12.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.4 MB 19.4 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: psutil in ./.venv-sb3/lib/python3.9/site-packages (from stable-baselines3[extra]>=2.0.0a4) (7.0.0)\n",
            "Requirement already satisfied: rich in ./.venv-sb3/lib/python3.9/site-packages (from stable-baselines3[extra]>=2.0.0a4) (14.0.0)\n",
            "Collecting ale-py>=0.9.0\n",
            "  Downloading ale_py-0.11.0-cp39-cp39-macosx_13_0_arm64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 16.8 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.9.1 in ./.venv-sb3/lib/python3.9/site-packages (from stable-baselines3[extra]>=2.0.0a4) (2.19.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.10.0 in ./.venv-sb3/lib/python3.9/site-packages (from ale-py>=0.9.0->stable-baselines3[extra]>=2.0.0a4) (8.7.0)\n",
            "Requirement already satisfied: typing-extensions in ./.venv-sb3/lib/python3.9/site-packages (from ale-py>=0.9.0->stable-baselines3[extra]>=2.0.0a4) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in ./.venv-sb3/lib/python3.9/site-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3[extra]>=2.0.0a4) (0.0.4)\n",
            "Requirement already satisfied: zipp>=3.20 in ./.venv-sb3/lib/python3.9/site-packages (from importlib-metadata>=4.10.0->ale-py>=0.9.0->stable-baselines3[extra]>=2.0.0a4) (3.21.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in ./.venv-sb3/lib/python3.9/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (3.1.3)\n",
            "Requirement already satisfied: packaging in ./.venv-sb3/lib/python3.9/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (25.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in ./.venv-sb3/lib/python3.9/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (2.2.2)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in ./.venv-sb3/lib/python3.9/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in ./.venv-sb3/lib/python3.9/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (3.8)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in ./.venv-sb3/lib/python3.9/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (6.30.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in ./.venv-sb3/lib/python3.9/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (58.0.4)\n",
            "Requirement already satisfied: six>1.9 in ./.venv-sb3/lib/python3.9/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.venv-sb3/lib/python3.9/site-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (0.7.2)\n",
            "Requirement already satisfied: fsspec in ./.venv-sb3/lib/python3.9/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]>=2.0.0a4) (2025.3.2)\n",
            "Requirement already satisfied: networkx in ./.venv-sb3/lib/python3.9/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]>=2.0.0a4) (3.2.1)\n",
            "Requirement already satisfied: filelock in ./.venv-sb3/lib/python3.9/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]>=2.0.0a4) (3.18.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in ./.venv-sb3/lib/python3.9/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]>=2.0.0a4) (1.14.0)\n",
            "Requirement already satisfied: jinja2 in ./.venv-sb3/lib/python3.9/site-packages (from torch<3.0,>=2.3->stable-baselines3[extra]>=2.0.0a4) (3.1.6)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv-sb3/lib/python3.9/site-packages (from sympy>=1.13.3->torch<3.0,>=2.3->stable-baselines3[extra]>=2.0.0a4) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.venv-sb3/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a4) (3.0.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in ./.venv-sb3/lib/python3.9/site-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a4) (6.5.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in ./.venv-sb3/lib/python3.9/site-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a4) (4.57.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in ./.venv-sb3/lib/python3.9/site-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a4) (2.9.0.post0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv-sb3/lib/python3.9/site-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a4) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv-sb3/lib/python3.9/site-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a4) (3.2.3)\n",
            "Requirement already satisfied: cycler>=0.10 in ./.venv-sb3/lib/python3.9/site-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a4) (0.12.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in ./.venv-sb3/lib/python3.9/site-packages (from matplotlib->stable-baselines3[extra]>=2.0.0a4) (1.3.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./.venv-sb3/lib/python3.9/site-packages (from pandas->stable-baselines3[extra]>=2.0.0a4) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./.venv-sb3/lib/python3.9/site-packages (from pandas->stable-baselines3[extra]>=2.0.0a4) (2025.2)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv-sb3/lib/python3.9/site-packages (from rich->stable-baselines3[extra]>=2.0.0a4) (2.19.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in ./.venv-sb3/lib/python3.9/site-packages (from rich->stable-baselines3[extra]>=2.0.0a4) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in ./.venv-sb3/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a4) (0.1.2)\n",
            "Installing collected packages: pygame, opencv-python, ale-py\n",
            "Successfully installed ale-py-0.11.0 opencv-python-4.11.0.86 pygame-2.6.1\n",
            "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.1.1 is available.\n",
            "You should consider upgrading via the '/Users/chen/Projects/RL-stablebaseline-scientist/.venv-sb3/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!apt-get update && apt-get install ffmpeg freeglut3-dev xvfb  # For visualization\n",
        "!pip install \"stable-baselines3[extra]>=2.0.0a4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "U29X1-B-AIKE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "stable_baselines3.__version__='2.6.0'\n"
          ]
        }
      ],
      "source": [
        "import stable_baselines3\n",
        "\n",
        "print(f\"{stable_baselines3.__version__=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtY8FhliLsGm"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcX8hEcaUpR0"
      },
      "source": [
        "Stable-Baselines works on environments that follow the [gym interface](https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html).\n",
        "You can find a list of available environment [here](https://gym.openai.com/envs/#classic_control).\n",
        "\n",
        "It is also recommended to check the [source code](https://github.com/openai/gym) to learn more about the observation and action space of each env, as gym does not have a proper documentation.\n",
        "Not all algorithms can work with all action spaces, you can find more in this [recap table](https://stable-baselines.readthedocs.io/en/master/guide/algos.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BIedd7Pz9sOs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gym.__version__='1.1.1'\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "print(f\"{gym.__version__=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae32CtgzTG3R"
      },
      "source": [
        "The first thing you need to import is the RL model, check the documentation to know what you can use on which problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "R7tKaBFrTR0a"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0_8OQbOTTNT"
      },
      "source": [
        "The next thing you need to import is the policy class that will be used to create the networks (for the policy/value functions).\n",
        "This step is optional as you can directly use strings in the constructor:\n",
        "\n",
        "```PPO('MlpPolicy', env)``` instead of ```PPO(MlpPolicy, env)```\n",
        "\n",
        "Note that some algorithms like `SAC` have their own `MlpPolicy`, that's why using string for the policy is the recommened option."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ROUJr675TT01"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.ppo import MlpPolicy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RapkYvTXL7Cd"
      },
      "source": [
        "## Create the Gym env and instantiate the agent\n",
        "\n",
        "For this example, we will use CartPole environment, a classic control problem.\n",
        "\n",
        "\"A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. \"\n",
        "\n",
        "Cartpole environment: [https://gymnasium.farama.org/environments/classic_control/cart_pole/](https://gymnasium.farama.org/environments/classic_control/cart_pole/)\n",
        "\n",
        "![Cartpole](https://cdn-images-1.medium.com/max/1143/1*h4WTQNVIsvMXJTCpXm_TAw.gif)\n",
        "\n",
        "Note: vectorized environments allow to easily multiprocess training. In this example, we are using only one process, hence the DummyVecEnv.\n",
        "\n",
        "We chose the MlpPolicy because input of CartPole is a feature vector, not images.\n",
        "\n",
        "The type of action to use (discrete/continuous) will be automatically deduced from the environment action space\n",
        "\n",
        "\n",
        "Here we are using the [Proximal Policy Optimization](https://stable-baselines.readthedocs.io/en/master/modules/ppo2.html) algorithm, which is an Actor-Critic method: it uses a value function to improve the policy gradient descent (by reducing the variance).\n",
        "\n",
        "It combines ideas from [A2C](https://stable-baselines.readthedocs.io/en/master/modules/a2c.html) (having multiple workers and using an entropy bonus for exploration) and [TRPO](https://stable-baselines.readthedocs.io/en/master/modules/trpo.html) (it uses a trust region to improve stability and avoid catastrophic drops in performance).\n",
        "\n",
        "PPO is an on-policy algorithm, which means that the trajectories used to update the networks must be collected using the latest policy.\n",
        "It is usually less sample efficient than off-policy alorithms like [DQN](https://stable-baselines.readthedocs.io/en/master/modules/dqn.html), [SAC](https://stable-baselines.readthedocs.io/en/master/modules/sac.html) or [TD3](https://stable-baselines.readthedocs.io/en/master/modules/td3.html), but is much faster regarding wall-clock time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "pUWGZp3i9wyf"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "model = PPO(MlpPolicy, env, verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4efFdrQ7MBvl"
      },
      "source": [
        "We create a helper function to evaluate the agent:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "63M8mSKR-6Zt"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, num_episodes=100, deterministic=True):\n",
        "    \"\"\"\n",
        "    Evaluate a RL agent\n",
        "    :param model: (BaseRLModel object) the RL Agent\n",
        "    :param num_episodes: (int) number of episodes to evaluate it\n",
        "    :return: (float) Mean reward for the last num_episodes\n",
        "    \"\"\"\n",
        "    # This function will only work for a single Environment\n",
        "    vec_env = model.get_env()\n",
        "    all_episode_rewards = []\n",
        "    for i in range(num_episodes):\n",
        "        episode_rewards = []\n",
        "        done = False\n",
        "        obs = vec_env.reset()\n",
        "        while not done:\n",
        "            # _states are only useful when using LSTM policies\n",
        "            action, _states = model.predict(obs, deterministic=deterministic)\n",
        "            # here, action, rewards and dones are arrays\n",
        "            # because we are using vectorized env\n",
        "            # also note that the step only returns a 4-tuple, as the env that is returned\n",
        "            # by model.get_env() is an sb3 vecenv that wraps the >v0.26 API\n",
        "            obs, reward, done, info = vec_env.step(action)\n",
        "            episode_rewards.append(reward)\n",
        "\n",
        "        all_episode_rewards.append(sum(episode_rewards))\n",
        "\n",
        "    mean_episode_reward = np.mean(all_episode_rewards)\n",
        "    print(\"Mean reward:\", mean_episode_reward, \"Num episodes:\", num_episodes)\n",
        "\n",
        "    return mean_episode_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hkyafs--gJz"
      },
      "source": [
        "In fact, Stable-Baselines3 already provides you with that helper:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "s6ZNldIR-fce"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjEVOIY8NVeK"
      },
      "source": [
        "Let's evaluate the un-trained agent, this should be a random agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xDHLMA6NFk95"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/chen/Projects/RL-stablebaseline-scientist/.venv-sb3/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean_reward:30.60 +/- 8.54\n"
          ]
        }
      ],
      "source": [
        "# Use a separate environement for evaluation\n",
        "eval_env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "# Random Agent, before training\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5UoXTZPNdFE"
      },
      "source": [
        "## Train the agent and evaluate it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "e4cfSXIB-pTF"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<stable_baselines3.ppo.ppo.PPO at 0x10783bbe0>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train the agent for 10000 steps\n",
        "model.learn(total_timesteps=10_000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ygl_gVmV_QP7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mean_reward:409.55 +/- 109.39\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the trained agent\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=100)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A00W6yY3NkHG"
      },
      "source": [
        "Apparently the training went well, the mean reward increased a lot !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVm9QPNVwKXN"
      },
      "source": [
        "### Prepare video recording"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MPyfQxD5z26J"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sh: Xvfb: command not found\n"
          ]
        }
      ],
      "source": [
        "# Set up fake display; otherwise rendering will fail\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SLzXxO8VMD6N"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "from pathlib import Path\n",
        "\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "\n",
        "def show_videos(video_path=\"\", prefix=\"\"):\n",
        "    \"\"\"\n",
        "    Taken from https://github.com/eleurent/highway-env\n",
        "\n",
        "    :param video_path: (str) Path to the folder containing videos\n",
        "    :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
        "    \"\"\"\n",
        "    html = []\n",
        "    for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
        "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "        html.append(\n",
        "            \"\"\"<video alt=\"{}\" autoplay\n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>\"\"\".format(\n",
        "                mp4, video_b64.decode(\"ascii\")\n",
        "            )\n",
        "        )\n",
        "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTRNUfulOGaF"
      },
      "source": [
        "We will record a video using the [VecVideoRecorder](https://stable-baselines.readthedocs.io/en/master/guide/vec_envs.html#vecvideorecorder) wrapper, you will learn about those wrapper in the next notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Trag9dQpOIhx"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
        "\n",
        "\n",
        "def record_video(env_id, model, video_length=500, prefix=\"\", video_folder=\"videos/\"):\n",
        "    \"\"\"\n",
        "    :param env_id: (str)\n",
        "    :param model: (RL model)\n",
        "    :param video_length: (int)\n",
        "    :param prefix: (str)\n",
        "    :param video_folder: (str)\n",
        "    \"\"\"\n",
        "    eval_env = DummyVecEnv([lambda: gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")])\n",
        "    # Start the video at step=0 and record 500 steps\n",
        "    eval_env = VecVideoRecorder(\n",
        "        eval_env,\n",
        "        video_folder=video_folder,\n",
        "        record_video_trigger=lambda step: step == 0,\n",
        "        video_length=video_length,\n",
        "        name_prefix=prefix,\n",
        "    )\n",
        "\n",
        "    obs = eval_env.reset()\n",
        "    for _ in range(video_length):\n",
        "        action, _ = model.predict(obs)\n",
        "        obs, _, _, _ = eval_env.step(action)\n",
        "\n",
        "    # Close the video recorder\n",
        "    eval_env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOObbeu5MMlR"
      },
      "source": [
        "### Visualize trained agent\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "iATu7AiyMQW2"
      },
      "outputs": [
        {
          "ename": "DependencyNotInstalled",
          "evalue": "MoviePy is not installed, run `pip install 'gymnasium[other]'`",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "File \u001b[0;32m~/Projects/RL-stablebaseline-scientist/.venv-sb3/lib/python3.9/site-packages/stable_baselines3/common/vec_env/vec_video_recorder.py:77\u001b[0m, in \u001b[0;36mVecVideoRecorder.__init__\u001b[0;34m(self, venv, video_folder, record_video_trigger, video_length, name_prefix)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmoviepy\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'moviepy'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrecord_video\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCartPole-v1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mppo-cartpole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[15], line 14\u001b[0m, in \u001b[0;36mrecord_video\u001b[0;34m(env_id, model, video_length, prefix, video_folder)\u001b[0m\n\u001b[1;32m     12\u001b[0m eval_env \u001b[38;5;241m=\u001b[39m DummyVecEnv([\u001b[38;5;28;01mlambda\u001b[39;00m: gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCartPole-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Start the video at step=0 and record 500 steps\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m eval_env \u001b[38;5;241m=\u001b[39m \u001b[43mVecVideoRecorder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrecord_video_trigger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvideo_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvideo_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m obs \u001b[38;5;241m=\u001b[39m eval_env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(video_length):\n",
            "File \u001b[0;32m~/Projects/RL-stablebaseline-scientist/.venv-sb3/lib/python3.9/site-packages/stable_baselines3/common/vec_env/vec_video_recorder.py:79\u001b[0m, in \u001b[0;36mVecVideoRecorder.__init__\u001b[0;34m(self, venv, video_folder, record_video_trigger, video_length, name_prefix)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmoviepy\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mDependencyNotInstalled(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMoviePy is not installed, run `pip install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgymnasium[other]\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
            "\u001b[0;31mDependencyNotInstalled\u001b[0m: MoviePy is not installed, run `pip install 'gymnasium[other]'`"
          ]
        }
      ],
      "source": [
        "record_video(\"CartPole-v1\", model, video_length=500, prefix=\"ppo-cartpole\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-n4i-fW3NojZ"
      },
      "outputs": [],
      "source": [
        "show_videos(\"videos\", prefix=\"ppo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y8zg4V566qD"
      },
      "source": [
        "## Bonus: Train a RL Model in One Line\n",
        "\n",
        "The policy class to use will be inferred and the environment will be automatically created. This works because both are [registered](https://stable-baselines.readthedocs.io/en/master/guide/quickstart.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaOPfOrwWEP4"
      },
      "outputs": [],
      "source": [
        "model = PPO('MlpPolicy', \"CartPole-v1\", verbose=1).learn(1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrI6f5fWnzp-"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook we have seen:\n",
        "- how to define and train a RL model using stable baselines3, it takes only one line of code ;)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv-sb3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
